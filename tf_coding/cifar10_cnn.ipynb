{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy\n",
    "import sys\n",
    "height, width, dim = 32, 32, 3\n",
    "classes = 10\n",
    "\n",
    "# this function is provided from the official site\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "# from PIL import Image\n",
    "# def ndarray2image (arr_data, image_fn):\n",
    "#   img = Image.fromarray(arr_data, 'RGB')\n",
    "#   img.save(image_fn)\n",
    "\n",
    "# need pillow package\n",
    "from scipy.misc import imsave\n",
    "def ndarray2image (arr_data, image_fn):\n",
    "    imsave(image_fn, arr_data)\n",
    "\n",
    "def read_dataset(dataset_path, ouput_type):\n",
    "    # define the information of images which can be obtained from official website\n",
    "\n",
    "    ''' read training data '''\n",
    "    # get the file names which start with \"data_batch\" (training data)\n",
    "    train_fns = [fn for fn in listdir(dataset_path) if isfile(join(dataset_path, fn)) & fn.startswith(\"data_batch\")]\n",
    "\n",
    "    # list sorting\n",
    "    train_fns.sort()\n",
    "\n",
    "    # make a glace about the training data\n",
    "    fn = train_fns[0]\n",
    "    raw_data = unpickle(dataset_path + fn)\n",
    "\n",
    "    # type of raw data\n",
    "    type(raw_data)\n",
    "    # <type 'dict'>\n",
    "\n",
    "    # check keys of training data\n",
    "    raw_data_keys = raw_data.keys()\n",
    "    # output ['data', 'labels', 'batch_label', 'filenames']\n",
    "\n",
    "    # check dimensions of ['data']\n",
    "    raw_data['data'].shape\n",
    "    # (10000, 3072)\n",
    "\n",
    "    # concatenate pixel (px) data into one ndarray [img_px_values]\n",
    "    # concatenate label data into one ndarray [img_lab]\n",
    "    img_px_values = 0\n",
    "    img_lab = 0\n",
    "    for fn in train_fns:\n",
    "        raw_data = unpickle(dataset_path + fn)\n",
    "        if fn == train_fns[0]:\n",
    "            img_px_values = raw_data['data']\n",
    "            img_lab = raw_data['labels']\n",
    "        else:\n",
    "            img_px_values = numpy.vstack((img_px_values, raw_data['data']))\n",
    "            img_lab = numpy.hstack((img_lab, raw_data['labels']))\n",
    "\n",
    "    X_train = []\n",
    "    \n",
    "    if (ouput_type == \"vec\"):\n",
    "        # set X_train as 1d-ndarray (50000,3072)\n",
    "        X_train = img_px_values\n",
    "    elif (ouput_type == \"img\"):\n",
    "        # set X_train as 3d-ndarray (50000,32,32,3)\n",
    "        X_train = numpy.asarray([numpy.dstack((r[0:(width*height)].reshape(height,width),\n",
    "                                               r[(width*height):(2*width*height)].reshape(height,width),\n",
    "                                               r[(2*width*height):(3*width*height)].reshape(height,width)\n",
    "                                             )) for r in img_px_values])\n",
    "    else:\n",
    "        sys.exit(\"Error ouput_type\")\n",
    "\n",
    "    Y_train = numpy.array(img_lab)\n",
    "\n",
    "    # check is same or not!\n",
    "    # lab_eql = numpy.array_equal([(numpy.argmax(r)) for r in Y_train], numpy.array(img_lab))\n",
    "\n",
    "    # draw one image from the pixel data\n",
    "    if (ouput_type == \"img\"):\n",
    "        ndarray2image(X_train[0],\"test_image.png\")\n",
    "\n",
    "    # print the dimension of training data\n",
    "    print ('X_train shape:', X_train.shape)\n",
    "    print ('Y_train shape:', Y_train.shape)\n",
    "\n",
    "    ''' read testing data '''\n",
    "    # get the file names which start with \"test_batch\" (testing data)\n",
    "    test_fns = [fn for fn in listdir(dataset_path) if isfile(join(dataset_path, fn)) & fn.startswith(\"test_batch\")]\n",
    "\n",
    "    # read testing data\n",
    "    fn = test_fns[0]\n",
    "    raw_data = unpickle(dataset_path + fn)\n",
    "    print ('testing file', dataset_path + fn)\n",
    "\n",
    "    # type of raw data\n",
    "    type(raw_data)\n",
    "\n",
    "    # check keys of testing data\n",
    "    raw_data_keys = raw_data.keys()\n",
    "    # ['data', 'labels', 'batch_label', 'filenames']\n",
    "\n",
    "    img_px_values = raw_data['data']\n",
    "\n",
    "    # check dimensions of data\n",
    "    print (\"dim(data)\", numpy.array(img_px_values).shape)\n",
    "    # dim(data) (10000, 3072)\n",
    "\n",
    "    img_lab = raw_data['labels']\n",
    "    # check dimensions of labels\n",
    "    print (\"dim(labels)\",numpy.array(img_lab).shape)\n",
    "    # dim(data) (10000,)\n",
    "\n",
    "    if (ouput_type == \"vec\"):\n",
    "        X_test = img_px_values\n",
    "    elif (ouput_type == \"img\"):\n",
    "        X_test = numpy.asarray([numpy.dstack((r[0:(width*height)].reshape(height,width),\n",
    "                                              r[(width*height):(2*width*height)].reshape(height,width),\n",
    "                                              r[(2*width*height):(3*width*height)].reshape(height,width)\n",
    "                                            )) for r in img_px_values])\n",
    "    else:\n",
    "        sys.exit(\"Error ouput_type\")\n",
    "\n",
    "    Y_test = numpy.array(raw_data['labels'])\n",
    "\n",
    "    # scale image data to range [0, 1]\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255.0\n",
    "    X_test /= 255.0\n",
    "\n",
    "    # print the dimension of training data\n",
    "    print ('X_test shape:', X_test.shape)\n",
    "    print ('Y_test shape:', Y_test.shape)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "import csv\n",
    "def write_csv(output_fn, fit_log):\n",
    "    history_fn = output_fn + '.csv'\n",
    "    with open(history_fn, 'w') as csv_file:\n",
    "        w = csv.writer(csv_file, lineterminator='\\n')\n",
    "        temp = numpy.array(list(fit_log.history.values()))\n",
    "        w.writerow(list(fit_log.history.keys()))\n",
    "        for i in range(temp.shape[1]):\n",
    "            w.writerow(temp[:,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\silver\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import [package name] as [name abbr.]\n",
    "# python 處理數字跟 tensor 運算的主要套件\n",
    "import numpy as np\n",
    "# google 的 NN coding 套件\n",
    "import tensorflow as tf\n",
    "\n",
    "# 有可能會出現警告，但是可以不用理他\n",
    "# c:\\users\\silver\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36:\n",
    "# FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating`\n",
    "# is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
    "# from ._conv import register_converters as _register_converters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\silver\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000,)\n",
      "testing file ./cifar_10/test_batch\n",
      "dim(data) (10000, 3072)\n",
      "dim(labels) (10000,)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# set dataset path\n",
    "dataset_path = './cifar_10/'\n",
    "classes = 10\n",
    "X_train, X_test, Y_train, Y_test = read_dataset(dataset_path, \"img\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Define Model Structure (tf.leyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Input (x) and Output (y_),  y_ = f(x)\n",
    "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "y_ = tf.placeholder(tf.int32, [None])\n",
    "y_one = tf.one_hot(y_,classes)\n",
    "\n",
    "# convolutional part\n",
    "c1 = tf.layers.conv2d(x,filters=32,kernel_size=[3,3],strides=(1,1),padding='same')\n",
    "c1_relu = tf.nn.relu(c1)\n",
    "c2 = tf.layers.conv2d(c1_relu,filters=32,kernel_size=[3,3],strides=(1,1),padding='same')\n",
    "c2_relu = tf.nn.relu(c2)\n",
    "mxp = tf.layers.max_pooling2d(c2_relu,pool_size=[2,2],strides=(1,1))\n",
    "do = tf.layers.dropout(mxp,rate=0.5)\n",
    "\n",
    "flt = tf.layers.flatten(do)\n",
    "\n",
    "# fully connected part\n",
    "f1 = tf.layers.dense(flt,512,activation=None)\n",
    "f1_relu = tf.nn.relu(f1)\n",
    "y = tf.layers.dense(f1_relu,10,activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model Loss (4)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_one, logits=y))\n",
    "\n",
    "# Define the Optimizer (5)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "y_pred = tf.argmax(tf.nn.softmax(y), 1, output_type=tf.int32)\n",
    "\n",
    "# Accuracy of the Model\n",
    "correct_prediction = tf.equal(y_pred, y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# initialize the model\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size = 128):\n",
    "    # print ('shuffle training dataset')\n",
    "    batch_size = 128\n",
    "    idx = np.arange(len(X))    \n",
    "    while True:\n",
    "        numpy.random.shuffle(idx)\n",
    "        tb = int(len(X)/batch_size)\n",
    "        #print('total batches %d' % tb)\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*batch_size):((b_idx+1)*batch_size)]\n",
    "            t_batch_x = X[tar_idx]\n",
    "            t_batch_y = Y[tar_idx]\n",
    "            # print(b_idx, t_batch_x.shape, t_batch_y.shape)\n",
    "            yield t_batch_x, t_batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [T] 0.1160 / [V] 0.1300\n",
      "Accuracy: [T] 0.2660 / [V] 0.2820\n",
      "Accuracy: [T] 0.3280 / [V] 0.3080\n",
      "Accuracy: [T] 0.3700 / [V] 0.3560\n",
      "Accuracy: [T] 0.3720 / [V] 0.3820\n",
      "Accuracy: [T] 0.4040 / [V] 0.3740\n",
      "Accuracy: [T] 0.4040 / [V] 0.3960\n",
      "Accuracy: [T] 0.4340 / [V] 0.4240\n",
      "Accuracy: [T] 0.4200 / [V] 0.4000\n",
      "Accuracy: [T] 0.4340 / [V] 0.4360\n"
     ]
    }
   ],
   "source": [
    "batches = get_batch(X_train, Y_train, 128)\n",
    "# Train Model for 1000 steps\n",
    "for step in range(1000):\n",
    "    batch_xs, batch_ys = next(batches)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if (step % 100 == 0):\n",
    "        # get training accr\n",
    "        idx = np.arange(len(X_train))    \n",
    "        tb = int(len(X_train)/500)\n",
    "        acc_train= []\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*500):((b_idx+1)*500)]\n",
    "            t_batch_x = X_train[tar_idx]\n",
    "            t_batch_y = Y_train[tar_idx]\n",
    "        acc_train.append(sess.run(accuracy, feed_dict={x: t_batch_x, y_: t_batch_y}))\n",
    "        # get test accr\n",
    "        idx = np.arange(len(X_test))    \n",
    "        tb = int(len(X_test)/500)\n",
    "        acc_valid= []\n",
    "        for b_idx in range(tb):\n",
    "            tar_idx = idx[(b_idx*500):((b_idx+1)*500)]\n",
    "            t_batch_x = X_test[tar_idx]\n",
    "            t_batch_y = Y_test[tar_idx]\n",
    "        acc_valid.append(sess.run(accuracy, feed_dict={x: t_batch_x, y_: t_batch_y}))\n",
    "        \n",
    "        print(\"Accuracy: [T] %.4f / [V] %.4f\" % (np.mean(acc_train),np.mean(acc_valid)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
